{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiKAeoeG2Q5f"
      },
      "outputs": [],
      "source": [
        "# Instal pustaka yang dibutuhkan\n",
        "!pip install pandas numpy scikit-learn matplotlib seaborn\n",
        "!pip install torch # Torch sudah terinstal di Colab, tapi ini memastikan versi terbaru jika perlu\n",
        "!pip install transformers accelerate datasets\n",
        "!pip install nltk Sastrawi # Untuk Bahasa Indonesia"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unduh data NLTK\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "OKArJnBa2dFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import json # Tambahkan ini untuk menyimpan label_mapping\n",
        "\n",
        "# --- 1. Muat Dataset (Simulasi) ---\n",
        "data = {\n",
        "    'Ulasan': [\n",
        "        \"Aplikasi ini sangat membantu sekali, navigasinya akurat!\",\n",
        "        \"Driver sering nyasar, bikin telat terus. Payah!\",\n",
        "        \"Fitur barunya lumayan, tapi loadingnya agak lambat.\",\n",
        "        \"Harga kadang naik terlalu tinggi saat jam sibuk.\",\n",
        "        \"Mudah digunakan dan responsif, suka sekali!\",\n",
        "        \"Pesanan saya dibatalkan sepihak, kecewa berat.\",\n",
        "        \"Oke lah, tidak ada keluhan khusus.\",\n",
        "        \"Pelayanan bagus, ramah-ramah drivernya.\",\n",
        "        \"Promo makin sedikit, jadi kurang menarik.\",\n",
        "        \"Antarmuka perlu diperbarui, terlalu jadul.\",\n",
        "        \"Selalu jadi andalan kalau mau bepergian.\",\n",
        "        \"Notifikasi sering terlambat masuk, jadi gak update.\",\n",
        "        \"Lumayan, tapi sering error kalau bayar pakai dompet digital.\",\n",
        "        \"Cepat sampai dan drivernya profesional.\",\n",
        "        \"Kenapa rating saya tiba-tiba turun padahal tidak ada masalah?\",\n",
        "        \"Mantap jiwa! Aplikasi terbaik sejauh ini.\",\n",
        "        \"Saya tidak suka dengan sistem rating baru mereka.\",\n",
        "        \"Netral saja, tidak ada yang spesial.\",\n",
        "        \"Kurang puas dengan fitur chat driver.\",\n",
        "        \"Gila sih, sering banget dapat promo diskon!\"\n",
        "    ],\n",
        "    'Sentimen': [\n",
        "        \"Positif\", \"Negatif\", \"Netral\", \"Negatif\", \"Positif\",\n",
        "        \"Negatif\", \"Netral\", \"Positif\", \"Negatif\", \"Negatif\",\n",
        "        \"Positif\", \"Negatif\", \"Netral\", \"Positif\", \"Negatif\",\n",
        "        \"Positif\", \"Negatif\", \"Netral\", \"Negatif\", \"Positif\"\n",
        "    ]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(\"--- Data Awal ---\")\n",
        "display(df.head())\n",
        "print(\"\\nDistribusi Sentimen:\")\n",
        "display(df['Sentimen'].value_counts())\n",
        "\n",
        "# --- 2. Pra-pemrosesan Teks ---\n",
        "factory = StopWordRemoverFactory()\n",
        "stop_words_remover = factory.create_stop_word_remover()\n",
        "stop_words_id = stop_words_remover.get_stopwords() # Corrected method call\n",
        "custom_stop_words = set(stop_words_id)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    # Remove punctuation and numbers, keep Indonesian letters and spaces\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_tokens = [word for word in tokens if word not in custom_stop_words]\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "df['cleaned_ulasan'] = df['Ulasan'].apply(preprocess_text)\n",
        "print(\"\\n--- Data Setelah Preprocessing ---\")\n",
        "display(df.head())\n",
        "\n",
        "# --- 3. Encoding Label Sentimen ---\n",
        "le = LabelEncoder()\n",
        "df['sentimen_encoded'] = le.fit_transform(df['Sentimen'])\n",
        "label_mapping = list(le.classes_)\n",
        "print(f\"\\nLabel Encoding Mapping: {label_mapping}\")\n",
        "display(df.head()) # Display after encoding\n",
        "\n",
        "# --- 4. Pembagian Data ---\n",
        "X = df['cleaned_ulasan']\n",
        "y = df['sentimen_encoded']\n",
        "\n",
        "# Adjusted test_size and random_state for better split with small dataset\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y) # Increased temp size for validation/test\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp) # Split temp into val and test\n",
        "\n",
        "print(f\"\\nUkuran Training Set: {len(X_train)}\")\n",
        "print(f\"Ukuran Validation Set: {len(X_val)}\")\n",
        "print(f\"Ukuran Test Set: {len(X_test)}\")\n",
        "\n",
        "# Ensure X_train, X_val, X_test, y_train, y_val, y_test are accessible for the next steps\n",
        "# These variables are now defined in the global scope of the notebook after this cell runs successfully."
      ],
      "metadata": {
        "id": "BVZPHVxu2ld1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 7. Evaluasi Model pada Test Set ---\n",
        "print(\"\\n--- Evaluasi Model pada Test Set ---\")\n",
        "results = trainer.evaluate(test_dataset)\n",
        "print(f\"Hasil evaluasi pada test set: {results}\")\n",
        "\n",
        "# Generate classification report dan confusion matrix\n",
        "predictions_output = trainer.predict(test_dataset)\n",
        "predicted_labels = np.argmax(predictions_output.predictions, axis=-1)\n",
        "\n",
        "print(\"\\n--- Classification Report ---\")\n",
        "print(classification_report(y_test, predicted_labels, target_names=label_mapping))\n",
        "\n",
        "print(\"\\n--- Confusion Matrix ---\")\n",
        "cm = confusion_matrix(y_test, predicted_labels)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_mapping, yticklabels=label_mapping)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# --- 8. Simpan Model & Tokenizer ---\n",
        "# Di Google Colab, model akan disimpan di lingkungan virtual sementara.\n",
        "# Jika Anda ingin menyimpannya secara permanen, Anda perlu menyimpannya ke Google Drive.\n",
        "model_save_path = \"./model_sentimen_ulasan_aplikasi\" # Ini akan disimpan di sesi Colab saat ini\n",
        "trainer.save_model(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "print(f\"\\nModel dan tokenizer berhasil disimpan di: {model_save_path}\")\n",
        "\n",
        "# Simpan label mapping juga\n",
        "with open(f\"{model_save_path}/label_mapping.json\", \"w\") as f:\n",
        "    json.dump(label_mapping, f)\n",
        "print(f\"Label mapping berhasil disimpan di: {model_save_path}/label_mapping.json\")"
      ],
      "metadata": {
        "id": "Ps_PPbLf2_2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fad28dcf"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "from datasets import Dataset\n",
        "\n",
        "# --- 5. Tokenisasi Data ---\n",
        "# Load the tokenizer for a pre-trained model (e.g., 'bert-base-uncased' or a suitable Indonesian model)\n",
        "# Using 'indobenchmark/indobert-base-p1' as a suitable Indonesian model\n",
        "tokenizer = AutoTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
        "\n",
        "# Convert pandas Series to Datasets\n",
        "train_dataset = Dataset.from_pandas(pd.DataFrame({'text': X_train.tolist(), 'label': y_train.tolist()}))\n",
        "val_dataset = Dataset.from_pandas(pd.DataFrame({'text': X_val.tolist(), 'label': y_val.tolist()}))\n",
        "test_dataset = Dataset.from_pandas(pd.DataFrame({'text': X_test.tolist(), 'label': y_test.tolist()}))\n",
        "\n",
        "\n",
        "# Apply tokenization\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "\n",
        "# --- 6. Fine-tuning Model (menggunakan Transformer) ---\n",
        "# Load pre-trained model for sequence classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    'indobenchmark/indobert-base-p1',\n",
        "    num_labels=len(label_mapping)  # Jumlah kelas sentimen (Positif, Negatif, Netral)\n",
        ")\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # Output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=8,  # batch size per device during training\n",
        "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\"      # Evaluate at the end of each epoch\n",
        ")\n",
        "\n",
        "# Create Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=tokenized_train_dataset,         # training dataset\n",
        "    eval_dataset=tokenized_val_dataset,            # evaluation dataset\n",
        "    tokenizer=tokenizer,                 # tokenizer used in preprocessing\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer) # Data collator\n",
        ")\n",
        "\n",
        "# Start training\n",
        "print(\"\\n--- Memulai Training Model ---\")\n",
        "trainer.train()\n",
        "print(\"--- Training Selesai ---\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c8d0274"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "from datasets import Dataset\n",
        "\n",
        "# --- 5. Tokenisasi Data ---\n",
        "# Load the tokenizer for a pre-trained model (e.g., 'bert-base-uncased' or a suitable Indonesian model)\n",
        "# Using 'indobenchmark/indobert-base-p1' as a suitable Indonesian model\n",
        "tokenizer = AutoTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
        "\n",
        "# Convert pandas Series to Datasets\n",
        "train_dataset = Dataset.from_pandas(pd.DataFrame({'text': X_train.tolist(), 'label': y_train.tolist()}))\n",
        "val_dataset = Dataset.from_pandas(pd.DataFrame({'text': X_val.tolist(), 'label': y_val.tolist()}))\n",
        "test_dataset = Dataset.from_pandas(pd.DataFrame({'text': X_test.tolist(), 'label': y_test.tolist()}))\n",
        "\n",
        "\n",
        "# Apply tokenization\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "\n",
        "# --- 6. Fine-tuning Model (menggunakan Transformer) ---\n",
        "# Load pre-trained model for sequence classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    'indobenchmark/indobert-base-p1',\n",
        "    num_labels=len(label_mapping)  # Jumlah kelas sentimen (Positif, Negatif, Netral)\n",
        ")\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # Output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=8,  # batch size per device during training\n",
        "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\"      # Evaluate at the end of each epoch\n",
        ")\n",
        "\n",
        "# Create Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=tokenized_train_dataset,         # training dataset\n",
        "    eval_dataset=tokenized_val_dataset,            # evaluation dataset\n",
        "    tokenizer=tokenizer,                 # tokenizer used in preprocessing\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer) # Data collator\n",
        ")\n",
        "\n",
        "# Start training\n",
        "print(\"\\n--- Memulai Training Model ---\")\n",
        "trainer.train()\n",
        "print(\"--- Training Selesai ---\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3836e70b"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory # Keep import in case other parts of Sastrawi are used later\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import json # Tambahkan ini untuk menyimpan label_mapping\n",
        "\n",
        "# --- 1. Muat Dataset (Simulasi) ---\n",
        "data = {\n",
        "    'Ulasan': [\n",
        "        \"Aplikasi ini sangat membantu sekali, navigasinya akurat!\",\n",
        "        \"Driver sering nyasar, bikin telat terus. Payah!\",\n",
        "        \"Fitur barunya lumayan, tapi loadingnya agak lambat.\",\n",
        "        \"Harga kadang naik terlalu tinggi saat jam sibuk.\",\n",
        "        \"Mudah digunakan dan responsif, suka sekali!\",\n",
        "        \"Pesanan saya dibatalkan sepihak, kecewa berat.\",\n",
        "        \"Oke lah, tidak ada keluhan khusus.\",\n",
        "        \"Pelayanan bagus, ramah-ramah drivernya.\",\n",
        "        \"Promo makin sedikit, jadi kurang menarik.\",\n",
        "        \"Antarmuka perlu diperbarui, terlalu jadul.\",\n",
        "        \"Selalu jadi andalan kalau mau bepergian.\",\n",
        "        \"Notifikasi sering terlambat masuk, jadi gak update.\",\n",
        "        \"Lumayan, tapi sering error kalau bayar pakai dompet digital.\",\n",
        "        \"Cepat sampai dan drivernya profesional.\",\n",
        "        \"Kenapa rating saya tiba-tiba turun padahal tidak ada masalah?\",\n",
        "        \"Mantap jiwa! Aplikasi terbaik sejauh ini.\",\n",
        "        \"Saya tidak suka dengan sistem rating baru mereka.\",\n",
        "        \"Netral saja, tidak ada yang spesial.\",\n",
        "        \"Kurang puas dengan fitur chat driver.\",\n",
        "        \"Gila sih, sering banget dapat promo diskon!\"\n",
        "    ],\n",
        "    'Sentimen': [\n",
        "        \"Positif\", \"Negatif\", \"Netral\", \"Negatif\", \"Positif\",\n",
        "        \"Negatif\", \"Netral\", \"Positif\", \"Negatif\", \"Negatif\",\n",
        "        \"Positif\", \"Negatif\", \"Netral\", \"Positif\", \"Negatif\",\n",
        "        \"Positif\", \"Negatif\", \"Netral\", \"Negatif\", \"Positif\"\n",
        "    ]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(\"--- Data Awal ---\")\n",
        "display(df.head())\n",
        "print(\"\\nDistribusi Sentimen:\")\n",
        "display(df['Sentimen'].value_counts())\n",
        "\n",
        "# --- 2. Pra-pemrosesan Teks ---\n",
        "# Use NLTK Indonesian stopwords as Sastrawi is causing issues\n",
        "stop_words_id = stopwords.words('indonesian')\n",
        "custom_stop_words = set(stop_words_id)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    # Remove punctuation and numbers, keep Indonesian letters and spaces\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_tokens = [word for word in tokens if word not in custom_stop_words]\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "df['cleaned_ulasan'] = df['Ulasan'].apply(preprocess_text)\n",
        "print(\"\\n--- Data Setelah Preprocessing ---\")\n",
        "display(df.head())\n",
        "\n",
        "# --- 3. Encoding Label Sentimen ---\n",
        "le = LabelEncoder()\n",
        "df['sentimen_encoded'] = le.fit_transform(df['Sentimen'])\n",
        "label_mapping = list(le.classes_)\n",
        "print(f\"\\nLabel Encoding Mapping: {label_mapping}\")\n",
        "display(df.head()) # Display after encoding\n",
        "\n",
        "# --- 4. Pembagian Data ---\n",
        "X = df['cleaned_ulasan']\n",
        "y = df['sentimen_encoded']\n",
        "\n",
        "# Adjusted test_size and random_state for better split with small dataset\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y) # Increased temp size for validation/test\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42) # Removed stratify from the second split\n",
        "\n",
        "print(f\"\\nUkuran Training Set: {len(X_train)}\")\n",
        "print(f\"Ukuran Validation Set: {len(X_val)}\")\n",
        "print(f\"Ukuran Test Set: {len(X_test)}\")\n",
        "\n",
        "# Ensure X_train, X_val, X_test, y_train, y_val, y_test are accessible for the next steps\n",
        "# These variables are now defined in the global scope of the notebook after this cell runs successfully."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "304d23b4"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "from datasets import Dataset\n",
        "\n",
        "# --- 5. Tokenisasi Data ---\n",
        "# Load the tokenizer for a pre-trained model (e.g., 'bert-base-uncased' or a suitable Indonesian model)\n",
        "# Using 'indobenchmark/indobert-base-p1' as a suitable Indonesian model\n",
        "tokenizer = AutoTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
        "\n",
        "# Convert pandas Series to Datasets\n",
        "train_dataset = Dataset.from_pandas(pd.DataFrame({'text': X_train.tolist(), 'label': y_train.tolist()}))\n",
        "val_dataset = Dataset.from_pandas(pd.DataFrame({'text': X_val.tolist(), 'label': y_val.tolist()}))\n",
        "test_dataset = Dataset.from_pandas(pd.DataFrame({'text': X_test.tolist(), 'label': y_test.tolist()}))\n",
        "\n",
        "\n",
        "# Apply tokenization\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "\n",
        "# --- 6. Fine-tuning Model (menggunakan Transformer) ---\n",
        "# Load pre-trained model for sequence classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    'indobenchmark/indobert-base-p1',\n",
        "    num_labels=len(label_mapping)  # Jumlah kelas sentimen (Positif, Negatif, Netral)\n",
        ")\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # Output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=8,  # batch size per device during training\n",
        "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\"      # Evaluate at the end of each epoch\n",
        ")\n",
        "\n",
        "# Create Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=tokenized_train_dataset,         # training dataset\n",
        "    eval_dataset=tokenized_val_dataset,            # evaluation dataset\n",
        "    tokenizer=tokenizer,                 # tokenizer used in preprocessing\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer) # Data collator\n",
        ")\n",
        "\n",
        "# Start training\n",
        "print(\"\\n--- Memulai Training Model ---\")\n",
        "trainer.train()\n",
        "print(\"--- Training Selesai ---\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb7c9c5a"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "from datasets import Dataset\n",
        "\n",
        "# --- 5. Tokenisasi Data ---\n",
        "# Load the tokenizer for a pre-trained model (e.g., 'bert-base-uncased' or a suitable Indonesian model)\n",
        "# Using 'indobenchmark/indobert-base-p1' as a suitable Indonesian model\n",
        "tokenizer = AutoTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
        "\n",
        "# Convert pandas Series to Datasets\n",
        "train_dataset = Dataset.from_pandas(pd.DataFrame({'text': X_train.tolist(), 'label': y_train.tolist()}))\n",
        "val_dataset = Dataset.from_pandas(pd.DataFrame({'text': X_val.tolist(), 'label': y_val.tolist()}))\n",
        "test_dataset = Dataset.from_pandas(pd.DataFrame({'text': X_test.tolist(), 'label': y_test.tolist()}))\n",
        "\n",
        "\n",
        "# Apply tokenization\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "\n",
        "# --- 6. Fine-tuning Model (menggunakan Transformer) ---\n",
        "# Load pre-trained model for sequence classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    'indobenchmark/indobert-base-p1',\n",
        "    num_labels=len(label_mapping)  # Jumlah kelas sentimen (Positif, Negatif, Netral)\n",
        ")\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # Output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=8,  # batch size per device during training\n",
        "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"epoch\"      # Evaluate at the end of each epoch # Corrected argument name\n",
        ")\n",
        "\n",
        "# Create Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=tokenized_train_dataset,         # training dataset\n",
        "    eval_dataset=tokenized_val_dataset,            # evaluation dataset\n",
        "    tokenizer=tokenizer,                 # tokenizer used in preprocessing\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer) # Data collator\n",
        ")\n",
        "\n",
        "# Start training\n",
        "print(\"\\n--- Memulai Training Model ---\")\n",
        "trainer.train()\n",
        "print(\"--- Training Selesai ---\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e28bdae1"
      },
      "source": [
        "# --- 7. Evaluasi Model pada Test Set ---\n",
        "print(\"\\n--- Evaluasi Model pada Test Set ---\")\n",
        "results = trainer.evaluate(tokenized_test_dataset) # Use tokenized_test_dataset\n",
        "print(f\"Hasil evaluasi pada test set: {results}\")\n",
        "\n",
        "# Generate classification report dan confusion matrix\n",
        "predictions_output = trainer.predict(tokenized_test_dataset) # Use tokenized_test_dataset\n",
        "predicted_labels = np.argmax(predictions_output.predictions, axis=-1)\n",
        "\n",
        "print(\"\\n--- Classification Report ---\")\n",
        "print(classification_report(y_test, predicted_labels, target_names=label_mapping))\n",
        "\n",
        "print(\"\\n--- Confusion Matrix ---\")\n",
        "cm = confusion_matrix(y_test, predicted_labels)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_mapping, yticklabels=label_mapping)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# --- 8. Simpan Model & Tokenizer ---\n",
        "# Di Google Colab, model akan disimpan di lingkungan virtual sementara.\n",
        "# Jika Anda ingin menyimpannya secara permanen, Anda perlu menyimpannya ke Google Drive.\n",
        "model_save_path = \"./model_sentimen_ulasan_aplikasi\" # Ini akan disimpan di sesi Colab saat ini\n",
        "trainer.save_model(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "print(f\"\\nModel dan tokenizer berhasil disimpan di: {model_save_path}\")\n",
        "\n",
        "# Simpan label mapping juga\n",
        "with open(f\"{model_save_path}/label_mapping.json\", \"w\") as f:\n",
        "    json.dump(label_mapping, f)\n",
        "print(f\"Label mapping berhasil disimpan di: {model_save_path}/label_mapping.json\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}